{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Transformer Encoder Block","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport math\nimport torch.nn.functional as f","metadata":{"execution":{"iopub.status.busy":"2023-07-22T12:56:20.074293Z","iopub.execute_input":"2023-07-22T12:56:20.074822Z","iopub.status.idle":"2023-07-22T12:56:20.081584Z","shell.execute_reply.started":"2023-07-22T12:56:20.074781Z","shell.execute_reply":"2023-07-22T12:56:20.080060Z"},"trusted":true},"execution_count":221,"outputs":[]},{"cell_type":"code","source":"class PositionwiseFeedForward(nn.Module):\n    def __init__(self,d_model,hidden,dropout=0.1):\n        super(PositionwiseFeedForward,self).__init__()\n        self.linear1=nn.Linear(d_model,hidden) # 512*512\n        self.linear2=nn.Linear(hidden,d_model) #2048*512\n        self.relu=nn.ReLU()\n        self.dropout=nn.Dropout(p=dropout)\n        \n    def forward(self,x): #32*100*512\n        x=self.linear1(x) #32*100*2048\n        x=self.relu(x)#32*100*2048\n        x=self.dropout(x)#32*100*2048\n        x=self.linear2(x)#32*100*512\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-07-22T12:56:20.139205Z","iopub.execute_input":"2023-07-22T12:56:20.139716Z","iopub.status.idle":"2023-07-22T12:56:20.149286Z","shell.execute_reply.started":"2023-07-22T12:56:20.139674Z","shell.execute_reply":"2023-07-22T12:56:20.148076Z"},"trusted":true},"execution_count":222,"outputs":[]},{"cell_type":"code","source":"class LayerNormalization(nn.Module):\n    def __init__(self,parameters_shape,eps=1e-5):\n        super().__init__()\n        self.parameters_shape=parameters_shape #512\n        self.eps=eps\n        self.gamma=nn.Parameter(torch.ones(parameters_shape)) # 512\n        self.beta=nn.Parameter(torch.zeros(parameters_shape))# 512\n         \n    def forward(self,inputs): # 32*100*512\n        dims=[(-i+1) for i in rnage(len(self.parameters_shape))] #[-1]\n        mean=inut.mean(dim=dims,keepdim=True) # 32*512*1\n        var=((inputs-mean)**2).mean(dim=dims,keepdim=True) # 32*100*1\n        std=(var+self.eps).sqrt()  # 32*100*1 \n        y=(inputs-mean)/std #32*100*512\n        out=self.gamma*y+self.beta # 32*100*512\n        return out","metadata":{"execution":{"iopub.status.busy":"2023-07-22T12:56:20.158080Z","iopub.execute_input":"2023-07-22T12:56:20.159079Z","iopub.status.idle":"2023-07-22T12:56:20.170548Z","shell.execute_reply.started":"2023-07-22T12:56:20.159030Z","shell.execute_reply":"2023-07-22T12:56:20.169111Z"},"trusted":true},"execution_count":223,"outputs":[]},{"cell_type":"code","source":"class EncoderLayer(nn.Module):\n    def __init__(self,d_model,ffn_hidden,nums_heads,dropout):\n        super(EncoderLayer,self).__init__()\n        self.attention=MultiHeadAttention(d_model=d_model,num_heads=num_heads)\n        self.norm1=LayerNormalization(parameters_shape=[d_model])\n        self.dropout1=nn.Dropout(p=dropout)\n        self.ffn=PositionwiseFeedForward(d_model=d_model,hidden=ffn_hidden,dropout=dropout)\n        self.norm2=LayerNormalization(parameters_shape=[d_model])\n        self.dropout2=nn.Dropout(p=dropout)\n        \n    def forward(self,x):\n        residual_x=x # 32*100*512\n        x=self.attention(x,mask=None)# 32*100*512\n        x=self.dropout1(x)#32*100*512\n        x=self.norm1(x+residual_x) #32*100*512\n        residual_x=x #32*100*512\n        x=self.ffn(x) # 32*100*512\n        x=self.dropout2(x)# 32*100*512\n        x=self.norm2(x+residual_x)# 32*100*512\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-07-22T12:56:20.183137Z","iopub.execute_input":"2023-07-22T12:56:20.183582Z","iopub.status.idle":"2023-07-22T12:56:20.194335Z","shell.execute_reply.started":"2023-07-22T12:56:20.183550Z","shell.execute_reply":"2023-07-22T12:56:20.192872Z"},"trusted":true},"execution_count":224,"outputs":[]},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self,d_model,ffn_hidden,num_heads,num_layers,dropout):\n        super().__init__()\n        self.layers=nn.Sequential(*[EncoderLayer(d_model,ffn_hidden,num_heads,dropout) for _ in range(num_layers)])\n    def forward(self,x):\n        x=self.layers(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-07-22T12:56:20.205020Z","iopub.execute_input":"2023-07-22T12:56:20.205616Z","iopub.status.idle":"2023-07-22T12:56:20.214934Z","shell.execute_reply.started":"2023-07-22T12:56:20.205575Z","shell.execute_reply":"2023-07-22T12:56:20.213494Z"},"trusted":true},"execution_count":225,"outputs":[]},{"cell_type":"code","source":"def ScaleDotProduct(q,k,v,mask=None):\n    #q,k,v=32*8*100*64\n    d_k=q.size()[-1] # 64\n    scaled=torch.matmul(q,k.transpose[-1,-2])/math.sqrt(d_k) # 32*8*100*100\n    if(mask is not None):\n        scaled+=mask # 100*100\n    attention=f.softmax(scaled,dim=-1) # 32*8*100*100\n    values=torch.matmul(attention,v) # 32*8*100*64\n    return values,attention  \n    ","metadata":{"execution":{"iopub.status.busy":"2023-07-22T12:56:20.227074Z","iopub.execute_input":"2023-07-22T12:56:20.228312Z","iopub.status.idle":"2023-07-22T12:56:20.236425Z","shell.execute_reply.started":"2023-07-22T12:56:20.228221Z","shell.execute_reply":"2023-07-22T12:56:20.235415Z"},"trusted":true},"execution_count":226,"outputs":[]},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self,d_model,num_heads):\n        super().__init__()\n        self.d_model=d_model # 512\n        self.head_dim=num_heads # 8\n        self.head_dim=d_model//num_heads # 64\n        self.qkv_layer=nn.Linear(d_model,3*d_model) # 512*1536\n        self.linear_layer=nn.Linear(d_model,d_model) # 512*512\n        \n    def forward(self,x,mask=None):\n        batch_size,seq_len,d_model=x.size() # 32*100*512\n        qkv=self.qkv_layer(x) # 32*100*1536\n        qkv=qkv.reshape(batch_size,seq_len,self.num_heads,3*self.head_dim) # 32*100*8*192\n        qkv=qkv.permute(0,2,1,3) # 32*8*100*192\n        q,k,v=qkv.chunk(3,dim=-1) # 32*8*100*(192/3)\n        values=values.reshape(batch_size,seq_le,self.num_heads*self.head_dim)\n        values,attenton=ScaleDotProduct(q,k,v,mask) # 32*8*100*64\n        values=values.reshape(batch_size,seq_len,self.num_heads*self.head_dim) # 32*100*512\n        out=self.linear_layer(values)\n        return out","metadata":{"execution":{"iopub.status.busy":"2023-07-22T12:56:20.246506Z","iopub.execute_input":"2023-07-22T12:56:20.247285Z","iopub.status.idle":"2023-07-22T12:56:20.258300Z","shell.execute_reply.started":"2023-07-22T12:56:20.247223Z","shell.execute_reply":"2023-07-22T12:56:20.257356Z"},"trusted":true},"execution_count":227,"outputs":[]},{"cell_type":"code","source":"d_model=512\nnum_heads=8\ndropout=0.1\nbatch_size=32\nmax_seq_len=200\nffn_hidden=2048\nnum_layers=5\n\nencoder=Encoder(d_model,ffn_hidden,num_heads,num_layers,dropout)","metadata":{"execution":{"iopub.status.busy":"2023-07-22T12:56:20.308564Z","iopub.execute_input":"2023-07-22T12:56:20.309376Z","iopub.status.idle":"2023-07-22T12:56:20.470832Z","shell.execute_reply.started":"2023-07-22T12:56:20.309323Z","shell.execute_reply":"2023-07-22T12:56:20.469632Z"},"trusted":true},"execution_count":228,"outputs":[]},{"cell_type":"markdown","source":"## Transformer Decoder block","metadata":{}},{"cell_type":"code","source":"class PositionwiseFeedForwarnetwork(nn.Module):\n    def __init__(self,d_model,hidden,dropout=0.1):\n        super(PositionwiseFeedForwarnetwork,self).__init__()\n        self.linear1=nn.Linear(d_model,hidden) # 512*512\n        self.linear2=nn.Linear(hidden,d_model) #2048*512\n        self.relu=nn.ReLU()\n        self.dropout=nn.Dropout(p=dropout)\n        \n    def forward(self,x): #32*100*512\n        x=self.linear1(x) #32*100*2048\n        x=self.relu(x) #32*100*2048\n        x=self.dropout(x)#32*100*2048\n        x=self.linear2(x)#32*100*512\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-07-22T12:56:20.476172Z","iopub.execute_input":"2023-07-22T12:56:20.476609Z","iopub.status.idle":"2023-07-22T12:56:20.485165Z","shell.execute_reply.started":"2023-07-22T12:56:20.476574Z","shell.execute_reply":"2023-07-22T12:56:20.483705Z"},"trusted":true},"execution_count":229,"outputs":[]},{"cell_type":"code","source":"class LayerNormalization(nn.Module):\n    def __init__(self,parameters_shape,eps=1e-5):\n        super().__init__()\n        self.parameters_shape=parameters_shape #512\n        self.eps=eps\n        self.gamma=nn.Parameter(torch.ones(parameters_shape)) # 512\n        self.beta=nn.Parameter(torch.zeros(parameters_shape))# 512\n         \n    def forward(self,inputs): # 32*200*512\n        dims=[(-i+1) for i in range(len(self.parameters_shape))] #[-1]\n        mean=inputs.mean(dim=dims,keepdim=True) # 32*512*1\n        var=((inputs-mean)**2).mean(dim=dims,keepdim=True) # 32*200*1\n        std=(var+self.eps).sqrt()  # 32*200*1 \n        y=(inputs-mean)/std #32*200*512\n        out=self.gamma*y+self.beta # 32*200*512\n        return out","metadata":{"execution":{"iopub.status.busy":"2023-07-22T12:56:20.486839Z","iopub.execute_input":"2023-07-22T12:56:20.487354Z","iopub.status.idle":"2023-07-22T12:56:20.501273Z","shell.execute_reply.started":"2023-07-22T12:56:20.487300Z","shell.execute_reply":"2023-07-22T12:56:20.500032Z"},"trusted":true},"execution_count":230,"outputs":[]},{"cell_type":"code","source":"def scaled_dot_product(q,k,v,mask=None):\n    #q,k,v->(30*8*200*64)\n    d_k=q.size()[-1]\n    scaled=torch.matmul(q,k.transpose(-1,-2))/math.sqrt(d_k)# 32*8*200*200\n    if mask is not None:\n        scaled+=mask#32*8*200*200\n    attention=f.softmax(scaled,dim=-1) # 30*8*200*200\n    values=torch.matmul(attention,v)#30*8*200*64\n    return values,attention","metadata":{"execution":{"iopub.status.busy":"2023-07-22T12:56:20.504473Z","iopub.execute_input":"2023-07-22T12:56:20.504888Z","iopub.status.idle":"2023-07-22T12:56:20.519440Z","shell.execute_reply.started":"2023-07-22T12:56:20.504853Z","shell.execute_reply":"2023-07-22T12:56:20.517955Z"},"trusted":true},"execution_count":231,"outputs":[]},{"cell_type":"code","source":"class MultiheadAttention(nn.Module):\n    def __init__(self,d_model,num_heads):\n        super().__init__()\n        self.d_model=d_model\n        self.num_heads=num_heads\n        self.head_dim=d_model//num_heads\n        self.qkv_layer=nn.Linear(d_model,3*d_model) #1536\n        self.linear_layer=nn.Linear(d_model,d_model)\n        \n    def forward(self,c,mask=None):\n        batch_size,seq_len,d_model=x.size() # 32*200*512\n        qkv=self.qkv_layer(x)#30*200*1536\n        qkv=qkv.reshape(batch_size,seq_len,self.num_heads,3*self.head_dim) #30*200*8*192\n        qkv=qkv.permute(0,2,1,3) #32*8*200*192\n        q,k,v=qkv.chunk(3,dim=-1) #30*8*200*64<-q,k,v\n        values,attention=scaled_dot_product(q,k,v,mask)#32*8*200*64\n        values=values.reshape(batch_size,seq_len,self.num_heads*self.head_dim)# 30*200*512\n        return out\n        ","metadata":{"execution":{"iopub.status.busy":"2023-07-22T12:56:20.521539Z","iopub.execute_input":"2023-07-22T12:56:20.521953Z","iopub.status.idle":"2023-07-22T12:56:20.534940Z","shell.execute_reply.started":"2023-07-22T12:56:20.521917Z","shell.execute_reply":"2023-07-22T12:56:20.534035Z"},"trusted":true},"execution_count":232,"outputs":[]},{"cell_type":"code","source":"class MultiHeadCrossAttention(nn.Module):\n    def __init__(self,d_model,num_heads):\n        super().__init__()\n        self.d_model=d_model\n        self.num_heads=num_heads\n        self.head_dim=d_model//num_heads\n        self.kv_layer=nn.Linear(d_model,2*d_model)# 1024\n        self.q_layer=nn.Linear(d_model,d_model)\n        self.linear_layer=nn.Linear(d_model,d_model)\n        \n    def forward(self,x,y,mask=None):\n        batch_size,seq_len,d_model=x.size()# 32*200*512\n        kv=self.kv_layer(x)# 32*200*1024\n        q=self.q_layer(y)#32*200*512\n        kv=kv.reshape(batch_size,seq_len,self.num_heads,2*self.head_dim)#32*200*8*128\n        q=q.reshape(batch_size,seq_len,self.num_heads,self.head_dim)# 32*200*8*64\n        kv=kv.permute(0,2,1,3)# 32*8*200*128\n        q=q.permute(0,2,1,3) # 32*8*200*64\n        k,v=kv.chunk(2,dim=-1)# 32*8*200*64\n        values,attention=scaled_dot_product(q,k,v,mask)\n        values=values.reshape(batch_size,seq_len,d_model)\n        return values","metadata":{"execution":{"iopub.status.busy":"2023-07-22T12:56:20.536493Z","iopub.execute_input":"2023-07-22T12:56:20.536857Z","iopub.status.idle":"2023-07-22T12:56:20.552276Z","shell.execute_reply.started":"2023-07-22T12:56:20.536827Z","shell.execute_reply":"2023-07-22T12:56:20.550872Z"},"trusted":true},"execution_count":233,"outputs":[]},{"cell_type":"code","source":"class DecoderLayer(nn.Module):\n    def __init__(self,d_model,ffn_hidden,num_heads,dropout):\n        super(DecoderLayer,self).__init__()\n        \n        self.self_attention=MultiheadAttention(d_model=d_model,num_heads=num_heads)\n        self.norm1=LayerNormalization(parameters_shape=[d_model])\n        self.dropout1=nn.Dropout(p=dropout)\n        \n        self.encoder_decoder_attention=MultiHeadCrossAttention(d_model=d_model,num_heads=num_heads)\n        self.norm2=LayerNormalization(parameters_shape=[d_model])\n        self.dropout2=nn.Dropout(p=dropout)\n        \n        self.ffn=PositionwiseFeedForwarnetwork(d_model,hidden=ffn_hidden,dropout=dropout)\n        self.norm3=LayerNormalization(parameters_shape=[d_model])\n        self.dropout3=nn.Dropout(p=dropout)\n        \n    def forward(self,x,y,decoder_mask):\n        _y=y\n        y=self.self_attention(y,mask=decoder_mask)#32*200*512\n        y=self.dropout1(y)#32*200*512\n        y=self.norm1(y+_y)#32*200*512\n\n        _y=y#32*200*512\n        y=self.encoder_decoder_attention(x,y,mask=None)\n        y=self.dropout2(y)\n        y=self.norm2(y+_y)\n        return y","metadata":{"execution":{"iopub.status.busy":"2023-07-22T12:56:20.553802Z","iopub.execute_input":"2023-07-22T12:56:20.554211Z","iopub.status.idle":"2023-07-22T12:56:20.572570Z","shell.execute_reply.started":"2023-07-22T12:56:20.554173Z","shell.execute_reply":"2023-07-22T12:56:20.571135Z"},"trusted":true},"execution_count":234,"outputs":[]},{"cell_type":"code","source":"class SequentialDecoder(nn.Sequential):\n    def forward(self,*inputs):\n        x,y,mask=inputs\n        for module in self._modules.values():\n            y=module(x,y,mask)\n        return y","metadata":{"execution":{"iopub.status.busy":"2023-07-22T12:56:20.573996Z","iopub.execute_input":"2023-07-22T12:56:20.574483Z","iopub.status.idle":"2023-07-22T12:56:20.591671Z","shell.execute_reply.started":"2023-07-22T12:56:20.574448Z","shell.execute_reply":"2023-07-22T12:56:20.590483Z"},"trusted":true},"execution_count":235,"outputs":[]},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self,d_model,ffn_hidden,num_heads,dropout,num_layers=1):\n        super().__init__()\n        self.layers=SequentialDecoder(*[DecoderLayer(d_model,ffn_hidden,num_heads,dropout) for _ in range(num_layers)])\n        \n    def forward(self,x,y,mask):\n        #x 32*200*512\n        #y 32*200*512\n        #mask 200*200 \n        y=self.layers(x,y,mask)\n        return y","metadata":{"execution":{"iopub.status.busy":"2023-07-22T12:56:20.593470Z","iopub.execute_input":"2023-07-22T12:56:20.593900Z","iopub.status.idle":"2023-07-22T12:56:20.604627Z","shell.execute_reply.started":"2023-07-22T12:56:20.593863Z","shell.execute_reply":"2023-07-22T12:56:20.603442Z"},"trusted":true},"execution_count":236,"outputs":[]},{"cell_type":"code","source":"d_model=512\nnum_heads=8\ndropout=0.1\nbatch_size=30\nmax_seq_len=200\nffn_hidden=20148\nnum_layers=5\n\nx=torch.randn((batch_size,max_seq_len,d_model))\ny=torch.randn((batch_size,max_seq_len,d_model))\nmask=torch.full([max_seq_len,max_seq_len],float('1e-9'))\nmask=torch.triu(mask,diagonal=1)\ndecoder=Decoder(d_model,ffn_hidden,num_heads,dropout,num_layers)\nout=decoder(x,y,mask)","metadata":{"execution":{"iopub.status.busy":"2023-07-22T12:56:20.608308Z","iopub.execute_input":"2023-07-22T12:56:20.608984Z","iopub.status.idle":"2023-07-22T12:56:24.797905Z","shell.execute_reply.started":"2023-07-22T12:56:20.608944Z","shell.execute_reply":"2023-07-22T12:56:24.796627Z"},"trusted":true},"execution_count":237,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}